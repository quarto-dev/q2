---
title: "JSON Serialization Format"
---

## Overview

`quarto-markdown` serializes source location information using a **pool-and-reference** system that minimizes JSON size while preserving complete provenance chains.

## The Problem

Naive serialization of SourceInfo creates massive JSON files. Consider 100 YAML keys sharing the same parent:

```json
// Without pooling: Parent chain duplicated 100 times
{
  "key1": {
    "mapping": "Substring",
    "parent": {
      "mapping": "Substring",
      "parent": {
        "mapping": "Original",
        "file_id": 0,
        "start": 0,
        "end": 1000
      },
      "start": 4,
      "end": 500
    },
    "start": 10,
    "end": 20
  }
}
// ... duplicated 99 more times
```

For real documents, this causes **25-55x size blowup** compared to the original source.

## The Solution: Pool-and-Reference

Store each unique SourceInfo once in a pool, reference it by ID:

```json
{
  "sourceInfoPool": [
    {"r": [0, 1000], "t": 0, "d": 0},     // ID 0: Original file
    {"r": [4, 500], "t": 1, "d": 0},      // ID 1: YAML frontmatter (parent=0)
    {"r": [10, 20], "t": 1, "d": 1},      // ID 2: First key (parent=1)
    {"r": [25, 35], "t": 1, "d": 1}       // ID 3: Second key (parent=1)
    // ... 97 more entries
  ]
}

// In the AST:
{"key1_source": 2, "key2_source": 3, ...}
```

Result: **~93% size reduction**

## Compact Encoding

Each pool entry uses ultra-compact field names:

```json
{"r": [start, end], "t": type_code, "d": type_data}
```

### Fields

- **`r`** (range): Array `[start_offset, end_offset]`
  - Always exactly 2 elements
  - Byte offsets (not character positions)
  - For Original: absolute offsets in source file
  - For Substring: relative offsets within parent

- **`t`** (type): Integer type code
  - `0` = Original
  - `1` = Substring
  - `2` = Concat
  - `3` = Transformed (deprecated, read as Substring)

- **`d`** (data): Type-specific data (varies by type code)

## Type Code Reference

### Type 0: Original

**Represents**: Text directly from a source file

**Format**:
```json
{"r": [start_offset, end_offset], "t": 0, "d": file_id}
```

**Data field**: `file_id` (integer) - Index into `astContext.files` array

**Example**:
```json
{"r": [0, 11], "t": 0, "d": 0}
```

Meaning: Bytes 0-11 of file #0 (the first file in the files array).

### Type 1: Substring

**Represents**: Text extracted from a parent source

**Format**:
```json
{"r": [start_offset, end_offset], "t": 1, "d": parent_id}
```

**Data field**: `parent_id` (integer) - Index of parent in sourceInfoPool

**Offsets**: Relative to the parent's text (not absolute file offsets)

**Example**:
```json
[
  {"r": [0, 100], "t": 0, "d": 0},    // ID 0: Original file, bytes 0-100
  {"r": [10, 30], "t": 1, "d": 0}     // ID 1: Substring of 0, bytes 10-30
]
```

ID 1 represents bytes 10-30 *within* ID 0's range. To find the absolute position:
- Parent (ID 0) starts at byte 0 in file
- Substring starts at offset 10 within parent
- Absolute position: 0 + 10 = byte 10 in file

### Type 2: Concat

**Represents**: Multiple sources joined together

**Format**:
```json
{"r": [0, total_length], "t": 2, "d": [[id1, off1, len1], [id2, off2, len2], ...]}
```

**Data field**: Array of pieces, where each piece is:
- `[source_info_id, offset_in_concat, length]`

**Range**: Always starts at 0, ends at sum of all piece lengths

**Example**:
```json
[
  {"r": [0, 5], "t": 0, "d": 0},      // ID 0: "Hello" (5 bytes)
  {"r": [6, 11], "t": 0, "d": 0},     // ID 1: "world" (5 bytes)
  {"r": [0, 10], "t": 2, "d": [[0, 0, 5], [1, 5, 5]]}  // ID 2: Concat
]
```

ID 2 joins two pieces:
- Piece 1: Source 0, starts at offset 0 in concat, length 5
- Piece 2: Source 1, starts at offset 5 in concat, length 5
- Total concatenated length: 10 bytes

## Pool Structure

The sourceInfoPool is an **array** where the index IS the ID:

```json
{
  "sourceInfoPool": [
    {"r": [0, 100], "t": 0, "d": 0},   // ID 0 (implicit)
    {"r": [10, 20], "t": 1, "d": 0},   // ID 1 (implicit)
    {"r": [30, 40], "t": 1, "d": 0}    // ID 2 (implicit)
  ]
}
```

No explicit `"id"` field needed - just use the array index.

### Topological Order

The pool **must** be in topological order: parents before children.

**Valid**:
```json
[
  {"r": [0, 100], "t": 0, "d": 0},    // ID 0: Original
  {"r": [10, 20], "t": 1, "d": 0}     // ID 1: References 0 (OK, 0 < 1)
]
```

**Invalid**:
```json
[
  {"r": [10, 20], "t": 1, "d": 1},    // ID 0: References 1 (ERROR: 1 >= 0)
  {"r": [0, 100], "t": 0, "d": 0}     // ID 1: Original
]
```

Parsers should reject pools with forward references.

## AST Node References

In the AST, the `"s"` field contains just the pool ID (integer):

```json
{
  "t": "Str",
  "c": "Hello",
  "s": 3
}
```

**Not** an object like `{"$ref": 3}` - just the number `3`.

## Complete Example: Sibling YAML Keys

This is the most common optimization case - multiple YAML keys sharing a parent:

**Source file** (`example.qmd`):
```yaml
---
title: Example
author: John Doe
date: 2024-01-15
---
```

**JSON Output**:
```json
{
  "astContext": {
    "files": [
      {
        "name": "example.qmd",
        "line_breaks": [3, 23, 43, 66],
        "total_length": 70
      }
    ],
    "sourceInfoPool": [
      {"r": [0, 70], "t": 0, "d": 0},      // ID 0: Entire file
      {"r": [4, 67], "t": 1, "d": 0},      // ID 1: YAML frontmatter (without delimiters)
      {"r": [0, 6], "t": 1, "d": 1},       // ID 2: "title" key substring
      {"r": [8, 15], "t": 1, "d": 1},      // ID 3: "Example" value substring
      {"r": [17, 23], "t": 1, "d": 1},     // ID 4: "author" key substring
      {"r": [25, 33], "t": 1, "d": 1},     // ID 5: "John Doe" value substring
      {"r": [35, 39], "t": 1, "d": 1},     // ID 6: "date" key substring
      {"r": [41, 51], "t": 1, "d": 1}      // ID 7: "2024-01-15" value substring
    ]
  },
  "meta": {
    "title": {
      "t": "MetaString",
      "c": "Example",
      "s": 3
    },
    "author": {
      "t": "MetaString",
      "c": "John Doe",
      "s": 5
    },
    "date": {
      "t": "MetaString",
      "c": "2024-01-15",
      "s": 7
    }
  },
  "metaTopLevelKeySources": {
    "title": 2,
    "author": 4,
    "date": 6
  }
}
```

### Size Comparison

**Without pooling**: Each of the 6 substrings would include its full parent chain:
- 6 entries × (1 Original + 1 YAML substring + 1 value substring)
- 18 total SourceInfo serializations
- ~1800 bytes

**With pooling**:
- 8 pool entries (1 Original + 1 YAML + 6 values)
- 9 references in AST (3 values + 6 key sources)
- ~300 bytes

**Reduction**: 83% smaller

## Complete Example: Deep Nesting

**Source**:
```markdown
---
metadata:
  nested:
    deep:
      value: "text"
---
```

**Pool** (showing parent chains):
```json
[
  {"r": [0, 100], "t": 0, "d": 0},       // ID 0: File
  {"r": [4, 95], "t": 1, "d": 0},        // ID 1: YAML (parent=0)
  {"r": [11, 90], "t": 1, "d": 1},       // ID 2: metadata value (parent=1)
  {"r": [11, 80], "t": 1, "d": 2},       // ID 3: nested value (parent=2)
  {"r": [8, 70], "t": 1, "d": 3},        // ID 4: deep value (parent=3)
  {"r": [8, 14], "t": 1, "d": 4}         // ID 5: "text" (parent=4)
]
```

Each level references its parent by ID. A 5-level chain needs only 6 pool entries, not 5×6=30 duplicated entries.

## Complete Example: Concat

**Source**: Three adjacent inline elements coalesced into a paragraph:

```markdown
Hello **world** today
```

**Pool**:
```json
[
  {"r": [0, 6], "t": 0, "d": 0},         // ID 0: "Hello "
  {"r": [8, 13], "t": 0, "d": 0},        // ID 1: "world"
  {"r": [16, 21], "t": 0, "d": 0},       // ID 2: " today"
  {"r": [0, 16], "t": 2, "d": [[0, 0, 6], [1, 6, 5], [2, 11, 5]]}  // ID 3: Full para
]
```

The Concat (ID 3) combines three pieces with their offsets in the concatenated result.

## Files Array

The `files` array provides metadata for mapping offsets to line/column:

```json
{
  "files": [
    {
      "name": "example.qmd",
      "line_breaks": [6, 13, 20],
      "total_length": 25
    }
  ]
}
```

### Fields

- **name**: Source file path (string)
- **line_breaks**: Byte offsets of each `\n` character (array of integers)
- **total_length**: File size in bytes (integer)

### Converting Offset to Line/Column

To convert byte offset 15 to (line, column):

1. **Find line**: Binary search `line_breaks` for largest offset ≤ 15
   - `line_breaks = [6, 13, 20]`
   - Offset 15 is after 13, before 20
   - Line = 2 (0-indexed) = line 3 (1-indexed for display)

2. **Find column**: Offset minus start of line
   - Line 2 starts at offset 14 (byte after line_breaks[1])
   - Column = 15 - 14 = 1 (0-indexed) = column 2 (1-indexed)

Result: Line 3, Column 2

## Backward Compatibility

### Old Range Format

The pool format supports an old 6-element range array for backward compatibility:

**New format** (current):
```json
{"r": [start_offset, end_offset], "t": 0, "d": 0}
```

**Old format** (deprecated):
```json
{"r": [start_off, start_row, start_col, end_off, end_row, end_col], "t": 0, "d": 0}
```

Parsers detect the array length:
- Length 2: New format (offsets only)
- Length 6: Old format (offsets + row/column, ignore row/column)

The old format is no longer written but must be supported when reading.

### Missing SourceInfo

For JSON generated before source tracking was added, nodes may lack `"s"` fields. Parsers should treat missing source info gracefully:

```rust
// Rust example
let source_info = node.get("s")
    .and_then(|v| deserializer.from_json_ref(v))
    .unwrap_or_else(SourceInfo::default);
```

## Validation Rules

Implementations should validate:

1. **Pool is array**: `sourceInfoPool` must be a JSON array
2. **Topological order**: No forward references (parent_id < current_index)
3. **Valid references**: All parent_id values exist in pool
4. **Range format**: Each entry has `r`, `t`, `d` fields
5. **Range length**: `r` is either 2 or 6 elements
6. **Type codes**: `t` is integer 0-3
7. **Concat pieces**: Each piece is 3-element array [id, offset, length]

Invalid pools should produce clear error messages:

```
Error: Circular reference in sourceInfoPool
  Entry 5 references parent 7, but 7 >= 5 (forward reference)
```

## Implementation Notes

### Why Compact Encoding?

Field names like `sourceInfoPool`, `start_offset`, `parent_id` add significant overhead:

**Verbose**:
```json
{
  "id": 0,
  "start_offset": 0,
  "end_offset": 100,
  "type": "Original",
  "file_id": 0
}
```
(80 characters)

**Compact**:
```json
{"r": [0, 100], "t": 0, "d": 0}
```
(31 characters)

**Savings**: 61% smaller per entry × hundreds of entries = significant reduction

### Why Topological Order?

Three alternatives considered:

1. **Require topological order** (chosen)
   - Pro: Simple linear deserialization
   - Pro: Easy validation (just check parent_id < current_id)
   - Con: Constrains serialization order

2. **Multiple passes**
   - Pro: Accept any order
   - Con: Slower deserialization (need multiple passes or complex graph building)
   - Con: Harder to detect circular references

3. **Reverse order** (children before parents)
   - Pro: Natural for depth-first traversal
   - Con: Requires full reversal before writing
   - Con: Confusing (IDs increase in reverse order)

Topological order is the best tradeoff: natural to produce, simple to validate, efficient to deserialize.

## Further Reading

- [Source Tracking Overview](index.qmd) - High-level architecture
- [Architecture Details](architecture.qmd) - Implementation internals
- [JSON Writer](../writers/json.qmd) - JSON output basics
